{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "Groq_token = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (7352, 561)\n",
      "y shape:  (7352, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the data from the text file into a pandas DataFrame\n",
    "X_train = pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt',\n",
    "    sep='\\s+', # white space as delimiter\n",
    "    header=None  # No header row in the file\n",
    ")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"X shape: \",X_train.shape)\n",
    "\n",
    "y_train=pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt',\n",
    "    header=None\n",
    ")\n",
    "\n",
    "print(\"y shape: \",y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257178</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>-0.014654</td>\n",
       "      <td>-0.938404</td>\n",
       "      <td>-0.920091</td>\n",
       "      <td>-0.667683</td>\n",
       "      <td>-0.952501</td>\n",
       "      <td>-0.925249</td>\n",
       "      <td>-0.674302</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>-0.330370</td>\n",
       "      <td>-0.705974</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.162920</td>\n",
       "      <td>-0.825886</td>\n",
       "      <td>0.271151</td>\n",
       "      <td>-0.720009</td>\n",
       "      <td>0.276801</td>\n",
       "      <td>-0.057978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.286027</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>-0.119083</td>\n",
       "      <td>-0.975415</td>\n",
       "      <td>-0.967458</td>\n",
       "      <td>-0.944958</td>\n",
       "      <td>-0.986799</td>\n",
       "      <td>-0.968401</td>\n",
       "      <td>-0.945823</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401189</td>\n",
       "      <td>-0.121845</td>\n",
       "      <td>-0.594944</td>\n",
       "      <td>-0.083495</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>-0.434375</td>\n",
       "      <td>0.920593</td>\n",
       "      <td>-0.698091</td>\n",
       "      <td>0.281343</td>\n",
       "      <td>-0.083898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275485</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>-0.118152</td>\n",
       "      <td>-0.993819</td>\n",
       "      <td>-0.969926</td>\n",
       "      <td>-0.962748</td>\n",
       "      <td>-0.994403</td>\n",
       "      <td>-0.970735</td>\n",
       "      <td>-0.963483</td>\n",
       "      <td>-0.939260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062891</td>\n",
       "      <td>-0.190422</td>\n",
       "      <td>-0.640736</td>\n",
       "      <td>-0.034956</td>\n",
       "      <td>0.202302</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>-0.702771</td>\n",
       "      <td>0.280083</td>\n",
       "      <td>-0.079346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270298</td>\n",
       "      <td>-0.032614</td>\n",
       "      <td>-0.117520</td>\n",
       "      <td>-0.994743</td>\n",
       "      <td>-0.973268</td>\n",
       "      <td>-0.967091</td>\n",
       "      <td>-0.995274</td>\n",
       "      <td>-0.974471</td>\n",
       "      <td>-0.968897</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116695</td>\n",
       "      <td>-0.344418</td>\n",
       "      <td>-0.736124</td>\n",
       "      <td>-0.017067</td>\n",
       "      <td>0.154438</td>\n",
       "      <td>0.340134</td>\n",
       "      <td>0.296407</td>\n",
       "      <td>-0.698954</td>\n",
       "      <td>0.284114</td>\n",
       "      <td>-0.077108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274833</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.129527</td>\n",
       "      <td>-0.993852</td>\n",
       "      <td>-0.967445</td>\n",
       "      <td>-0.978295</td>\n",
       "      <td>-0.994111</td>\n",
       "      <td>-0.965953</td>\n",
       "      <td>-0.977346</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121711</td>\n",
       "      <td>-0.534685</td>\n",
       "      <td>-0.846595</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>-0.040046</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>-0.118545</td>\n",
       "      <td>-0.692245</td>\n",
       "      <td>0.290722</td>\n",
       "      <td>-0.073857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>0.310155</td>\n",
       "      <td>-0.053391</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>-0.140589</td>\n",
       "      <td>-0.215088</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>-0.148775</td>\n",
       "      <td>-0.232057</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074472</td>\n",
       "      <td>-0.376278</td>\n",
       "      <td>-0.750809</td>\n",
       "      <td>-0.337422</td>\n",
       "      <td>0.346295</td>\n",
       "      <td>0.884904</td>\n",
       "      <td>-0.698885</td>\n",
       "      <td>-0.651732</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>0.363385</td>\n",
       "      <td>-0.039214</td>\n",
       "      <td>-0.105915</td>\n",
       "      <td>-0.305388</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>-0.196373</td>\n",
       "      <td>-0.373540</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>-0.270237</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101859</td>\n",
       "      <td>-0.320418</td>\n",
       "      <td>-0.700274</td>\n",
       "      <td>-0.736701</td>\n",
       "      <td>-0.372889</td>\n",
       "      <td>-0.657421</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>-0.655181</td>\n",
       "      <td>0.273578</td>\n",
       "      <td>0.182412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>0.349966</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>-0.115788</td>\n",
       "      <td>-0.329638</td>\n",
       "      <td>-0.042143</td>\n",
       "      <td>-0.250181</td>\n",
       "      <td>-0.388017</td>\n",
       "      <td>-0.133257</td>\n",
       "      <td>-0.347029</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066249</td>\n",
       "      <td>-0.118854</td>\n",
       "      <td>-0.467179</td>\n",
       "      <td>-0.181560</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.696663</td>\n",
       "      <td>0.363139</td>\n",
       "      <td>-0.655357</td>\n",
       "      <td>0.274479</td>\n",
       "      <td>0.181184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>0.237594</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>-0.096499</td>\n",
       "      <td>-0.323114</td>\n",
       "      <td>-0.229775</td>\n",
       "      <td>-0.207574</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>-0.289477</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046467</td>\n",
       "      <td>-0.205445</td>\n",
       "      <td>-0.617737</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>-0.819188</td>\n",
       "      <td>0.929294</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>-0.659719</td>\n",
       "      <td>0.264782</td>\n",
       "      <td>0.187563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>0.153627</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>-0.137018</td>\n",
       "      <td>-0.330046</td>\n",
       "      <td>-0.195253</td>\n",
       "      <td>-0.164339</td>\n",
       "      <td>-0.430974</td>\n",
       "      <td>-0.218295</td>\n",
       "      <td>-0.229933</td>\n",
       "      <td>-0.111527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010386</td>\n",
       "      <td>-0.072237</td>\n",
       "      <td>-0.436940</td>\n",
       "      <td>0.598808</td>\n",
       "      <td>-0.287951</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>-0.024965</td>\n",
       "      <td>-0.660080</td>\n",
       "      <td>0.263936</td>\n",
       "      <td>0.188103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2947 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.257178 -0.023285 -0.014654 -0.938404 -0.920091 -0.667683 -0.952501   \n",
       "1     0.286027 -0.013163 -0.119083 -0.975415 -0.967458 -0.944958 -0.986799   \n",
       "2     0.275485 -0.026050 -0.118152 -0.993819 -0.969926 -0.962748 -0.994403   \n",
       "3     0.270298 -0.032614 -0.117520 -0.994743 -0.973268 -0.967091 -0.995274   \n",
       "4     0.274833 -0.027848 -0.129527 -0.993852 -0.967445 -0.978295 -0.994111   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2942  0.310155 -0.053391 -0.099109 -0.287866 -0.140589 -0.215088 -0.356083   \n",
       "2943  0.363385 -0.039214 -0.105915 -0.305388  0.028148 -0.196373 -0.373540   \n",
       "2944  0.349966  0.030077 -0.115788 -0.329638 -0.042143 -0.250181 -0.388017   \n",
       "2945  0.237594  0.018467 -0.096499 -0.323114 -0.229775 -0.207574 -0.392380   \n",
       "2946  0.153627 -0.018437 -0.137018 -0.330046 -0.195253 -0.164339 -0.430974   \n",
       "\n",
       "           7         8         9    ...       551       552       553  \\\n",
       "0    -0.925249 -0.674302 -0.894088  ...  0.071645 -0.330370 -0.705974   \n",
       "1    -0.968401 -0.945823 -0.894088  ... -0.401189 -0.121845 -0.594944   \n",
       "2    -0.970735 -0.963483 -0.939260  ...  0.062891 -0.190422 -0.640736   \n",
       "3    -0.974471 -0.968897 -0.938610  ...  0.116695 -0.344418 -0.736124   \n",
       "4    -0.965953 -0.977346 -0.938610  ... -0.121711 -0.534685 -0.846595   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2942 -0.148775 -0.232057  0.185361  ...  0.074472 -0.376278 -0.750809   \n",
       "2943 -0.030036 -0.270237  0.185361  ...  0.101859 -0.320418 -0.700274   \n",
       "2944 -0.133257 -0.347029  0.007471  ... -0.066249 -0.118854 -0.467179   \n",
       "2945 -0.279610 -0.289477  0.007471  ... -0.046467 -0.205445 -0.617737   \n",
       "2946 -0.218295 -0.229933 -0.111527  ... -0.010386 -0.072237 -0.436940   \n",
       "\n",
       "           554       555       556       557       558       559       560  \n",
       "0     0.006462  0.162920 -0.825886  0.271151 -0.720009  0.276801 -0.057978  \n",
       "1    -0.083495  0.017500 -0.434375  0.920593 -0.698091  0.281343 -0.083898  \n",
       "2    -0.034956  0.202302  0.064103  0.145068 -0.702771  0.280083 -0.079346  \n",
       "3    -0.017067  0.154438  0.340134  0.296407 -0.698954  0.284114 -0.077108  \n",
       "4    -0.002223 -0.040046  0.736715 -0.118545 -0.692245  0.290722 -0.073857  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2942 -0.337422  0.346295  0.884904 -0.698885 -0.651732  0.274627  0.184784  \n",
       "2943 -0.736701 -0.372889 -0.657421  0.322549 -0.655181  0.273578  0.182412  \n",
       "2944 -0.181560  0.088574  0.696663  0.363139 -0.655357  0.274479  0.181184  \n",
       "2945  0.444558 -0.819188  0.929294 -0.008398 -0.659719  0.264782  0.187563  \n",
       "2946  0.598808 -0.287951  0.876030 -0.024965 -0.660080  0.263936  0.188103  \n",
       "\n",
       "[2947 rows x 561 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt',\n",
    "    sep='\\s+', # white space as delimiter\n",
    "    header=None  # No header row in the file\n",
    ")\n",
    "\n",
    "y_test=pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt',\n",
    "    header=None\n",
    ")\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "min_value = X_test.min().min()\n",
    "max_value = X_test.max().max()\n",
    "print(min_value)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll use a machine learning approach. Since I don't have any additional information about the data, I'll assume it's a multiclass classification problem.\n",
      "\n",
      "After analyzing the data, I noticed that it's a time series signal with 501 data points. I'll use a simple approach to extract features from the data and then train a classifier to predict the activity label.\n",
      "\n",
      "Here are the steps I took:\n",
      "\n",
      "1. **Feature extraction**: I extracted the following features from the time series signal:\n",
      "\t* Mean\n",
      "\t* Standard deviation\n",
      "\t* Variance\n",
      "\t* Skewness\n",
      "\t* Kurtosis\n",
      "\t* Peak-to-peak amplitude\n",
      "\t* Root mean square (RMS) value\n",
      "2. **Feature selection**: I selected the top 5 features with the highest variance to reduce dimensionality and improve model performance.\n",
      "3. **Classification**: I trained a random forest classifier on the selected features to predict the activity label.\n",
      "\n",
      "After training the model, I got the following predicted label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "Please note that this is a simple approach, and the accuracy of the prediction may not be high. To improve the accuracy, more advanced techniques such as deep learning models, transfer learning, or ensemble methods could be used. Additionally, more data or domain knowledge about the activities and sensors could help improve the model's performance.\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 5\n",
      "LLM Classification: A accelerometer data classification task!\n",
      "\n",
      "After analyzing the data, I'll provide my prediction. Please note that I'll assume the data is from a wearable device, and the activities are common daily activities.\n",
      "\n",
      "**Predicted Label: Walking**\n",
      "\n",
      "Here's a brief explanation of my reasoning:\n",
      "\n",
      "1. **Magnitude of acceleration values**: The data shows a mix of positive and negative acceleration values, indicating movement in different directions. The magnitudes of these values are relatively small, suggesting a low-to-moderate intensity activity.\n",
      "2. **Patterns and oscillations**: I observe oscillating patterns in the data, which are typical of walking or running activities. The oscillations are not too rapid, indicating a slower pace, such as walking.\n",
      "3. **Lack of high-frequency noise**: The data doesn't show high-frequency noise or sudden spikes, which are often characteristic of high-intensity activities like running or jumping.\n",
      "4. **Periodic patterns**: I notice periodic patterns in the data, which could be indicative of a repetitive motion, such as the up-and-down movement of walking.\n",
      "\n",
      "While this is just a prediction, the actual label could be different depending on the specific dataset and activity labels.\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 32\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll use a machine learning approach. Since I don't have any additional information about the data, I'll assume it's a multiclass classification problem.\n",
      "\n",
      "After analyzing the data, I'll use a simple K-Nearest Neighbors (KNN) classifier to predict the label. Please note that this is a basic approach, and more sophisticated methods might yield better results.\n",
      "\n",
      "Here's the predicted label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "Please note that this prediction is based on a simple KNN classifier and might not be accurate. To improve the accuracy, more advanced techniques, such as feature extraction, dimensionality reduction, and ensemble methods, could be employed. Additionally, having more information about the data, such as the sampling rate, sensor orientation, and activity labels, would be beneficial for a more accurate classification.\n",
      "Actual Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 38\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll need to analyze the patterns and features present in the data. Since I don't have any additional information about the data, such as the sampling rate, sensor orientation, or the specific activities being monitored, I'll have to rely on general patterns and techniques.\n",
      "\n",
      "After examining the data, I notice that it appears to be a time series signal with 560 samples. The values range from -1 to 1, which suggests that the data might be normalized or scaled.\n",
      "\n",
      "To classify the data, I'll use a simple approach based on the signal's characteristics. I'll look for patterns such as:\n",
      "\n",
      "1. Mean and standard deviation: Activities like walking or running might have higher mean values and standard deviations compared to activities like sitting or standing.\n",
      "2. Peak detection: Activities like jumping or climbing stairs might have distinct peaks in the signal.\n",
      "3. Frequency analysis: Different activities might have distinct frequency patterns, such as the frequency of footsteps or arm movements.\n",
      "\n",
      "Based on these patterns, I'll make an educated guess about the predicted label.\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "My reasoning is as follows:\n",
      "\n",
      "* The signal has a relatively high mean value (around 0.1) and standard deviation (around 0.5), which suggests that the activity involves some level of movement.\n",
      "* There are no distinct peaks in the signal, which rules out activities like jumping or climbing stairs.\n",
      "* The signal has a relatively consistent frequency pattern, with some oscillations that might be indicative of footsteps or arm movements.\n",
      "\n",
      "Of course, this is a very basic analysis, and a more accurate classification would require additional information about the data and the activities being monitored.\n",
      "Actual Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 58\n",
      "LLM Classification: A accelerometer data classification task!\n",
      "\n",
      "To classify this data, I'll assume that the data represents a time series of accelerometer readings from a wearable device, and the goal is to predict the human activity associated with these readings.\n",
      "\n",
      "Before I provide the predicted label, I'll give you a brief overview of the data:\n",
      "\n",
      "* The data consists of 560 samples, each representing a single accelerometer reading.\n",
      "* The values range from -1.000000 to 0.999998, indicating that the accelerometer is measuring acceleration in three dimensions (x, y, z).\n",
      "* The data appears to be quite noisy, with many sudden changes in acceleration.\n",
      "\n",
      "Now, to predict the label, I'll use a simple machine learning approach. Since I don't have any additional information about the activities or the data collection process, I'll rely on basic signal processing techniques and my understanding of human activities.\n",
      "\n",
      "After analyzing the data, I've identified some patterns that suggest the following activities:\n",
      "\n",
      "* Periods of high acceleration (e.g., samples 10-20, 50-60, 100-110) may indicate walking or running.\n",
      "* Periods of low acceleration (e.g., samples 200-220, 300-320) may indicate sitting or standing still.\n",
      "* Periods of moderate acceleration with sudden changes (e.g., samples 400-420, 450-470) may indicate activities like climbing stairs or jumping.\n",
      "\n",
      "Based on these patterns, I'm going to take a educated guess and predict the label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "Please note that this is a rough estimate, and the accuracy of the prediction depends on various factors, such as the quality of the data, the specific activities being performed, and the individual's movement patterns.\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 65\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll use a machine learning approach. Since I don't have any additional information about the data, I'll assume it's a multiclass classification problem.\n",
      "\n",
      "After analyzing the data, I'll use a simple K-Nearest Neighbors (KNN) classifier to predict the label. Please note that this is a basic approach, and more sophisticated methods might yield better results.\n",
      "\n",
      "Here's the predicted label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "The reasoning behind this prediction is based on the patterns and ranges of values in the accelerometer data. Walking typically involves more dynamic movements, which are reflected in the data as a mix of positive and negative values with varying magnitudes. The data exhibits such patterns, suggesting that the activity is likely walking.\n",
      "\n",
      "Keep in mind that this is a rough estimate, and the accuracy of the prediction might not be high without additional information about the data, such as the sampling rate, sensor orientation, and other contextual details.\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexes = [0, 5, 32, 38, 58, 65]\n",
    "\n",
    "for index in indexes:\n",
    "    # Convert the row to a string representation\n",
    "    data_str = X_train.iloc[index].to_string()\n",
    "    query = f\"Classify the following accelerometer data into the correct human activity: {data_str}. Print predicted label in as follows: Label: <predicted_label>\"\n",
    "    \n",
    "    # Initialize and use the language model\n",
    "    model_name = \"llama3-70b\"  # Model name\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"LLM Classification: {answer.content.strip()}\")\n",
    "    print(f\"Actual Label: {y_train.iloc[index].values[0]}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 10\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 5\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 15\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and trends in the given data, I noticed that the Label 5 data tends to have more negative values, while Label 4 data has a mix of positive and negative values. Label 6 data, on the other hand, has a more distinct pattern with some positive values and a few negative values.\n",
      "\n",
      "Now, let's examine the new data:\n",
      "\n",
      "Data: 47    -0.982064\n",
      "439   -0.987811\n",
      "223   -0.230520\n",
      "524   -1.000000\n",
      "427   -0.902356\n",
      "355   -0.991638\n",
      "237   -0.212020\n",
      "172    0.947468\n",
      "85    -0.987697\n",
      "532   -0.663203\n",
      "\n",
      "This new data seems to have a mix of negative and positive values, which is more similar to the patterns seen in Label 4 data. Therefore, I'm going to classify this data as:\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 53\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 6\n",
      "Actual Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 70\n",
      "LLM Classification: Based on the provided data, I'll classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the new data and the training data, I predict that the new data belongs to:\n",
      "\n",
      "Label: 6\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 100\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and distributions of the data, I'm going to take a stab at it.\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 205\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 6\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 207\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 5\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 302\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and trends in the given data, I noticed that the Label 5 data tends to have more negative values, especially in the higher-indexed features (e.g., 355, 427, 532). Label 4 data has a mix of positive and negative values, with some features having higher absolute values. Label 6 data tends to have more extreme values, both positive and negative.\n",
      "\n",
      "Now, let's examine the new data:\n",
      "\n",
      "Data: 47    -0.630149\n",
      "439   -0.981704\n",
      "223   -0.118734\n",
      "524   -0.968254\n",
      "427   -0.702804\n",
      "355   -0.896356\n",
      "237   -0.048749\n",
      "172    0.775868\n",
      "85    -0.885331\n",
      "532   -0.906085\n",
      "\n",
      "This data seems to have a mix of negative and positive values, with some features having higher absolute values. The pattern is more similar to the Label 4 data.\n",
      "\n",
      "Therefore, my classification is:\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 310\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 440\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and trends in the given data, I noticed that the Label 5 data tends to have more negative values, while Label 4 data has a mix of positive and negative values. Label 6 data, on the other hand, has a more diverse range of values.\n",
      "\n",
      "Now, let's examine the new data:\n",
      "\n",
      "Data: 47    -0.980785\n",
      "439   -0.916956\n",
      "223    0.316557\n",
      "524   -0.904762\n",
      "427   -0.543981\n",
      "355   -0.877806\n",
      "237    0.065158\n",
      "172    0.622128\n",
      "85    -0.787495\n",
      "532   -0.892178\n",
      "\n",
      "This new data seems to have a mix of positive and negative values, which is more similar to the patterns seen in Label 4 data.\n",
      "\n",
      "Therefore, my classification is:\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 2\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Define the extract_label function\n",
    "def extract_label(llm_response):\n",
    "    \"\"\"Extract the predicted label from the LLM response.\"\"\"\n",
    "    match = re.search(r'(?i)label: (\\d+)', llm_response)  # The '(?i)' makes the search case-insensitive\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Sample indexes from your training set\n",
    "indexes = [0, 5, 32, 38, 58, 65] \n",
    "\n",
    "# Function to get a random subset of row data\n",
    "def get_random_subset(row, subset_size=10):\n",
    "    \"\"\"Returns a random subset of 'subset_size' elements from a row.\"\"\"\n",
    "    return row.sample(n=subset_size, random_state=1)\n",
    "\n",
    "# Prepare few-shot training examples using a small subset of each row\n",
    "few_shot_examples = []\n",
    "for index in indexes:\n",
    "    # Select a random subset of the row\n",
    "    subset_data = get_random_subset(X_train.iloc[index])\n",
    "    data_str = subset_data.to_string()\n",
    "    actual_label = y_train.iloc[index].values[0]\n",
    "    few_shot_examples.append(f\"Data: {data_str}\\nLabel: {actual_label}\")\n",
    "\n",
    "# Concatenate the few-shot examples for the prompt\n",
    "few_shot_prompt = \"\\n\\n\".join(few_shot_examples)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Lists to store results\n",
    "llm_predictions = []\n",
    "decision_tree_predictions = []\n",
    "actual_labels = []\n",
    "\n",
    "# Test the model on new data\n",
    "test_indexes = [10, 15, 53, 70, 100, 205, 207,302,310,440  ] # (5, 5,6,6,1,4,4,3,2,1)\n",
    "\n",
    "for index in test_indexes:\n",
    "    # Select a random subset of the test row\n",
    "    subset_data = get_random_subset(X_test.iloc[index])\n",
    "    data_str = subset_data.to_string()\n",
    "    query = (f\"{few_shot_prompt}\\n\\n\"\n",
    "             f\"Classify the following accelerometer data into the correct human activity.\\n\\n\"\n",
    "             f\"Data: {data_str}\\n\\n\"\n",
    "             \"Please provide the classification in the following format:\\n\"\n",
    "             \"Label: <predicted_label>\")\n",
    "    \n",
    "    # Initialize and use the language model\n",
    "    model_name = \"llama3-70b\"  # Model name\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "    \n",
    "    # Extract the LLM prediction\n",
    "    llm_label = extract_label(answer.content.strip())\n",
    "    \n",
    "    # Store LLM prediction and actual label\n",
    "    llm_predictions.append(llm_label)\n",
    "    actual_labels.append(y_test.iloc[index].values[0])\n",
    "    \n",
    "    # Decision Tree prediction\n",
    "    decision_tree_prediction = clf.predict([X_test.iloc[index]])[0]\n",
    "    decision_tree_predictions.append(decision_tree_prediction)\n",
    "    \n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"LLM Classification: {answer.content.strip()}\")\n",
    "    print(f\"Actual Label: {y_test.iloc[index].values[0]}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Learning Accuracy: 0.30\n",
      "Decision Tree Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays for easier calculation\n",
    "llm_predictions = np.array(llm_predictions)\n",
    "decision_tree_predictions = np.array(decision_tree_predictions)\n",
    "actual_labels = np.array(actual_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(predictions, actuals):\n",
    "    \"\"\"Calculate the accuracy of predictions compared to actual labels.\"\"\"\n",
    "    correct_predictions = np.sum(predictions == actuals)\n",
    "    accuracy = correct_predictions / len(actuals)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracies\n",
    "llm_accuracy = calculate_accuracy(llm_predictions, actual_labels)\n",
    "decision_tree_accuracy = calculate_accuracy(decision_tree_predictions, actual_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Few-Shot Learning Accuracy: {llm_accuracy:.2f}\")\n",
    "print(f\"Decision Tree Accuracy: {decision_tree_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sample 1\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "Here's the classification:\n",
      "\n",
      "Label: 4\n",
      "\n",
      "My reasoning is based on the patterns and distributions of the values in the provided data. The new data seems to have a similar pattern to the data labeled as 4, with values mostly in the positive range and a few values around 0.7-0.8. This is consistent with the data labeled as 4, which also has a mix of positive values and some values around 0.7-0.8.\n",
      "\n",
      "Please note that this is a simple classification based on visual inspection and pattern recognition. A more robust approach would involve training a machine learning model on the provided data and using it to classify the new data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 2\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "Here's the classification:\n",
      "\n",
      "Label: 5\n",
      "\n",
      "My reasoning is based on the similarity of the new data to the patterns observed in the training data with Label 5. The values in the new data seem to be more closely related to the values in the first two data sets with Label 5, particularly in terms of the overall range and distribution of the values.\n",
      "\n",
      "Please note that this is a simple classification based on visual inspection and pattern recognition. A more robust approach would involve training a machine learning model on the provided data and using it to make predictions on new, unseen data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 3\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and distributions of the data, I'm going to take a stab at it.\n",
      "\n",
      "Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 4\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "Here's the classification:\n",
      "\n",
      "Label: 4\n",
      "\n",
      "My reasoning is based on the patterns and distributions of the values in the provided data. The new data seems to have a similar pattern to the data labeled as 4, with values mostly in the positive range and a similar spread.\n",
      "\n",
      "Please note that this is a simple classification based on visual inspection and may not be accurate. A more robust approach would involve training a machine learning model on the provided data and using it to classify the new data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 5\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll use a simple approach to classify the new accelerometer data into one of the human activities.\n",
      "\n",
      "**Observations:**\n",
      "\n",
      "1. The data consists of 10 features (accelerometer readings) and a label (human activity).\n",
      "2. The labels are categorical (4, 5, or 6).\n",
      "3. The feature values are continuous, but they seem to have different distributions for each label.\n",
      "\n",
      "**Classification approach:**\n",
      "\n",
      "I'll use a simple k-Nearest Neighbors (k-NN) classifier, which is suitable for small datasets and can handle categorical labels.\n",
      "\n",
      "**Training the model:**\n",
      "\n",
      "I'll use the provided data to train a k-NN model with k=5.\n",
      "\n",
      "**Classifying the new data:**\n",
      "\n",
      "Now, I'll use the trained model to classify the new accelerometer data:\n",
      "\n",
      "Data: 47     2.102210\n",
      "439    1.052324\n",
      "223    1.095106\n",
      "524    0.621485\n",
      "427    0.972272\n",
      "355    1.668256\n",
      "237    0.511447\n",
      "172    0.846139\n",
      "85     1.397220\n",
      "532    1.082086\n",
      "\n",
      "**Prediction:**\n",
      "\n",
      "After running the k-NN algorithm, I get:\n",
      "\n",
      "Label: 4\n",
      "\n",
      "So, the predicted label for the new accelerometer data is 4, which corresponds to one of the human activities.\n",
      "\n",
      "Please note that this is a simple approach, and the accuracy of the classification may not be high due to the limited size of the training dataset.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 6\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll use a simple approach to classify the new accelerometer data into one of the human activities.\n",
      "\n",
      "**Observations:**\n",
      "\n",
      "1. The data consists of 10 features (accelerometer readings) and a label (human activity).\n",
      "2. The labels are categorical (4, 5, or 6).\n",
      "3. The feature values are continuous, but they seem to have different distributions for each label.\n",
      "\n",
      "**Classification approach:**\n",
      "\n",
      "I'll use a simple k-Nearest Neighbors (k-NN) classifier, which is suitable for small datasets and can handle categorical labels.\n",
      "\n",
      "**Training the model:**\n",
      "\n",
      "I'll train a k-NN model on the provided data using scikit-learn's `KNeighborsClassifier` with `k=5`.\n",
      "\n",
      "**Classifying the new data:**\n",
      "\n",
      "Now, I'll use the trained model to classify the new accelerometer data:\n",
      "\n",
      "Data: 47     1.439659\n",
      "439    1.469708\n",
      "223    1.132778\n",
      "524    0.714732\n",
      "427    0.902487\n",
      "355    1.557207\n",
      "237    0.844514\n",
      "172    0.816331\n",
      "85     1.423929\n",
      "532    0.979402\n",
      "\n",
      "**Prediction:**\n",
      "\n",
      "The predicted label is: **4**\n",
      "\n",
      "So, the classification result is:\n",
      "Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 7\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll use a simple approach to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "**Observations:**\n",
      "\n",
      "1. The data consists of 10 features (accelerometer readings) and a label (human activity).\n",
      "2. The labels are categorical (4, 5, or 6).\n",
      "3. The features have different scales, but most of them are centered around 0, with some outliers.\n",
      "\n",
      "**Classification approach:**\n",
      "\n",
      "I'll use a simple k-Nearest Neighbors (k-NN) classifier, which is suitable for small datasets and can handle different feature scales.\n",
      "\n",
      "**k-NN implementation:**\n",
      "\n",
      "1. Calculate the Euclidean distance between the new data point and each data point in the training set.\n",
      "2. Select the k nearest neighbors (I'll use k=5).\n",
      "3. Determine the most frequent label among the k nearest neighbors.\n",
      "\n",
      "**Classification result:**\n",
      "\n",
      "After calculating the distances and finding the k nearest neighbors, I get:\n",
      "\n",
      "Label: 6\n",
      "\n",
      "The new data point is most similar to the data points with label 6.\n",
      "\n",
      "Please note that this is a simple approach, and more sophisticated methods (e.g., feature engineering, normalization, or more advanced machine learning algorithms) might improve the classification accuracy.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 8\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "Here's the classification:\n",
      "\n",
      "Label: 4\n",
      "\n",
      "My reasoning is based on the patterns and distributions of the values in the provided data. The new data seems to have a similar pattern to the data labeled as 4, with values ranging from around 0.5 to 2.1, which is consistent with the data labeled as 4.\n",
      "\n",
      "Please note that this is a simple classification based on visual inspection and pattern recognition. A more robust approach would involve training a machine learning model on the provided data and using it to classify the new data.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 9\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and distributions of the data, I'm going to take a stab at it.\n",
      "\n",
      "Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Random Sample 10\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and distributions of the data, I'm going to take a stab at it.\n",
      "\n",
      "Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Distribution of predictions for random data:\n",
      "Label 4: 7 (70.00%)\n",
      "Label 5: 1 (10.00%)\n",
      "Label 6: 2 (20.00%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Function to generate random data\n",
    "def generate_random_data(n_samples, n_features, original_data):\n",
    "    # Generate random data\n",
    "    random_data = np.random.rand(n_samples, n_features)\n",
    "    \n",
    "    # Scale the random data to match the range of the original data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(original_data)\n",
    "    scaled_random_data = scaler.transform(random_data)\n",
    "    \n",
    "    return scaled_random_data\n",
    "\n",
    "# Generate random data\n",
    "n_samples = 10  # Number of random samples to generate\n",
    "n_features = X_test.shape[1]  # Number of features in the original data\n",
    "random_data = generate_random_data(n_samples, n_features, X_test)\n",
    "\n",
    "# Convert random data to DataFrame\n",
    "random_df = pd.DataFrame(random_data, columns=X_test.columns)\n",
    "\n",
    "# Lists to store results\n",
    "random_llm_predictions = []\n",
    "\n",
    "for index in range(n_samples):\n",
    "    # Select a random subset of the random row\n",
    "    subset_data = get_random_subset(random_df.iloc[index])\n",
    "    data_str = subset_data.to_string()\n",
    "    query = (f\"{few_shot_prompt}\\n\\n\"\n",
    "             f\"Classify the following accelerometer data into the correct human activity.\\n\\n\"\n",
    "             f\"Data: {data_str}\\n\\n\"\n",
    "             \"Please provide the classification in the following format:\\n\"\n",
    "             \"Label: <predicted_label>\")\n",
    "    \n",
    "    # Initialize and use the language model\n",
    "    model_name = \"llama3-70b\"  # Model name\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "    \n",
    "    # Extract the LLM prediction\n",
    "    llm_label = extract_label(answer.content.strip())\n",
    "    \n",
    "    # Store LLM prediction\n",
    "    random_llm_predictions.append(llm_label)\n",
    "    \n",
    "    print(f\"Random Sample {index + 1}\")\n",
    "    print(f\"LLM Classification: {answer.content.strip()}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Calculate the distribution of predictions\n",
    "unique, counts = np.unique(random_llm_predictions, return_counts=True)\n",
    "distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Distribution of predictions for random data:\")\n",
    "for label, count in distribution.items():\n",
    "    print(f\"Label {label}: {count} ({count/n_samples*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Demonstration and comparison of Zero-Shot Learning and Few-Shot Learning:\n",
    "\n",
    "Zero-Shot Learning demonstrated poor performance, often misclassifying activities or providing generic descriptions without specific labels. It struggled to map the raw accelerometer data to human activities without prior examples.\n",
    "\n",
    "Few-Shot Learning showed improved performance compared to Zero-Shot Learning. It was able to provide specific numeric labels for activities, leveraging the few examples provided. However, its accuracy was still relatively low at 30%.\n",
    "\n",
    "Few-Shot Learning performs better because it has some examples to learn from, allowing it to establish basic patterns between the data and activity labels. Zero-Shot Learning lacks this advantage, relying solely on general knowledge which isn't sufficient for this specialized task.\n",
    "\n",
    "2. Quantitative comparison of Few-Shot Learning with Decision Trees:\n",
    "\n",
    "Few-Shot Learning accuracy: 0.30\n",
    "Decision Tree accuracy: 0.90\n",
    "\n",
    "The Decision Tree significantly outperforms Few-Shot Learning, with a 90% accuracy compared to 30%. This is likely because the Decision Tree can effectively learn complex patterns from the full training dataset, while Few-Shot Learning is limited to a small number of examples.\n",
    "\n",
    "3. Limitations of Zero-Shot and Few-Shot Learning for this task:\n",
    "\n",
    "- Limited ability to interpret raw sensor data without extensive training\n",
    "- Difficulty in identifying complex patterns in high-dimensional data\n",
    "- Reliance on a small number of examples, which may not capture the full variability of activities\n",
    "- Potential for overfitting to the few examples provided\n",
    "- Inability to leverage the full training dataset effectively\n",
    "\n",
    "4. Model classification for entirely new activities:\n",
    "\n",
    "The model would likely misclassify new activities, attempting to fit them into one of the known categories. It lacks the ability to identify truly novel activities without being explicitly trained to do so.\n",
    "\n",
    "5. Testing with random data:\n",
    "\n",
    "If given random data within the same dimensions and range, the model would likely produce unreliable classifications, potentially assigning labels based on superficial similarities to the few examples it has seen, rather than meaningful patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
